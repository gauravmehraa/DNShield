-------------------------
import pandas as pd
from sklearn.neighbors import NearestNeighbors

def find_similar_users(user_id, data, k, positions):
  k += 1
  user = data.loc[user_id].values.reshape(1, -1)
  knn = NearestNeighbors(n_neighbors=k, algorithm="brute", metric="cosine").fit(data)
  distances, indices = knn.kneighbors(user, return_distance=True)
  return distances[0][1:], [positions[i] for i in indices[0][1:]]

def predict(ratings, user_id, movie_id, k):
  data = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)
  users = ratings['userId'].unique()
  positions = dict(zip(range(len(users)), users))
  
  if data.loc[user_id][movie_id] != 0:
    print("Rating already exists")
    return data.loc[user_id][movie_id]
  
  distances, similar_users = find_similar_users(user_id, data, k, positions)
  numerator = denominator = 0
  for distance, other_user in zip(distances, similar_users):
    rating = data.loc[other_user, movie_id]
    if rating:
      numerator += rating * distance
      denominator += distance
  
  if denominator == 0:
    print("Insufficient Ratings")
    return None
  
  return numerator / denominator

data = {
  'userId':  [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10],
  'movieId': [1, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1,  3],
  'rating':  [4, 5, 5, 4, 3, 3, 2, 4, 4, 4, 2, 3, 5, 5, 2, 3, 4, 5, 4, 5, 3, 3, 3, 3,  4]
}

ratings = pd.DataFrame(data)
user_id, movie_id, k = 1, 3, 3

prediction = predict(ratings, user_id, movie_id, k)
print(f"Predicted Rating: {prediction}") # 3.0

-------------------------

from collections import defaultdict
import math

def get_tokens(tfs):
  tokens = set()
  for tf in tfs: tokens.update(tf.keys())
  return tokens

def get_logf(tf):
  logf = {}
  for token, freq in tf.items():
    logf[token] = 0.0 if freq == 0 else round((math.log10(freq) + 1), 2)
  return logf

def normalize(logf):
  factor = math.sqrt(sum(val * val for val in logf.values()))
  normalized = {}
  for token, val in logf.items():
    normalized[token] = round(val / factor, 2) if factor != 0 else 0.0
  return normalized

def build_vectors(normalized, tokens):
  vectors = []
  for norm in normalized:
    vector = [norm.get(token, 0.0) for token in tokens]
    vectors.append(vector)
  return vectors

def cosine_similarity(vec1, vec2):
  dot = sum(a * b for a, b in zip(vec1, vec2))
  norm1 = math.sqrt(sum(a * a for a in vec1))
  norm2 = math.sqrt(sum(b * b for b in vec2))
  if norm1 == 0 or norm2 == 0: return 0.0
  return round(dot / (norm1 * norm2), 2)

tfs = [
  { 'reaction': 15, 'params': 25, 'vehicle': 0, 'synthesize': 16, 'fast': 1 },
  { 'reaction': 0, 'params': 5, 'vehicle': 20, 'synthesize': 2, 'fast': 1 },
  { 'reaction': 3, 'params': 2 , 'vehicle': 11, 'synthesize': 0, 'fast': 19 },
]
n = len(tfs)

tokens = get_tokens(tfs)
logfs = [get_logf(tf) for tf in tfs]
normalized = [normalize(logf) for logf in logfs]
vectors = build_vectors(normalized, tokens)

for i in range(n):
  for j in range(i + 1, n):
    sim = cosine_similarity(vectors[i], vectors[j])
    print(f"Doc {i+1} and Doc {j+1}: {sim}")

Doc 1 and Doc 2: 0.59
Doc 1 and Doc 3: 0.59
Doc 2 and Doc 3: 0.76

-------------------------

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def crawl(url, depth=0, visited=None):
  if visited is None:
    visited = set()

  if url in visited: return
  print(f"Level {depth}: {url}")
  visited.add(url)
  try:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    for link in soup.find_all("a", href=True):
      if link["href"].startswith("#"): continue
      new_url = urljoin(url, link["href"])
      if not new_url.startswith(url): continue
      crawl(new_url, depth + 1, visited)
  except Exception as e: print("Error:", e)

if __name__ == "__main__":
  root = "https://mithibai.ac.in/"
  crawl(root)

-------------------------

from nltk.corpus import stopwords

# run once to download all stopwords 
# nltk.download('stopwords')

def stopword_removal(text):
  stopwords_set = set(stopwords.words('english'))
  cleaned = ''.join(char for char in text if char.isalnum() or char.isspace())
  tokens = cleaned.lower().split()
  filtered_tokens = [token for token in tokens if token not in stopwords_set]
  return ' '.join(filtered_tokens)

def incidence_matrix(texts):
  n = len(texts)
  matrix = [] # incidence matrix
  seen = set()
  vocab = [] # collection of all terms

  for i in range(n):
    texts[i] = stopword_removal(texts[i]).split()
    for token in texts[i]:
      if token not in seen: vocab.append(token)
      seen.add(token)
  
  positions = {} # for storing the row number of a term
  for row, term in enumerate(vocab):
    frequencies = [1 if term in texts[i] else 0 for i in range(n)] # represents a row of 0/1  
    matrix.append([term] + frequencies) # [term, 0, 1, ...]
    positions[term] = row # e.g. { term: 2 }

  return matrix, positions

def search(query, matrix, positions):
  n = len(matrix[0]) - 1
  terms = query.split()[::2]
  operators = query.split()[1::2]

  rows = [] # represents rows of the queried terms
  for term in terms:
    row = positions.get(term)
    if row is None: rows.append([0] * n) # term does not exist
    else: rows.append(matrix[row][1:]) # zero index is the term

  result = rows[0] # by default
  if len(rows) == 1: return result # only 1 term in query

  for index, row in enumerate(rows[1:]):
    operator = operators[index]
    for i in range(n): #  for each term
      if operator == "AND": result[i] &= row[i]
      elif operator == "OR": result[i] |= row[i]
  return result

texts = [
  "Alan Mathison Turing (23 June 1912 to 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science.",
  "Born in London, Turing was raised in southern England. He graduated from King's College, Cambridge, and in 1938, earned a doctorate degree from Princeton University. During World War II, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. He led Hut 8, the section responsible for German naval cryptanalysis. Turing devised techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. He played a crucial role in cracking intercepted messages that enabled the Allies to defeat the Axis powers in many engagements, including the Battle of the Atlantic.",
  "After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. Turing wrote on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, he was never fully recognised during his lifetime because much of his work was covered by the Official Secrets Act.",
  "In 1952, Turing was prosecuted for homosexual acts. He accepted hormone treatment, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, aged 41, from cyanide poisoning. An inquest determined his death as suicide, but the evidence is also consistent with accidental poisoning. Following a campaign in 2009, British prime minister Gordon Brown made an official public apology for 'the appalling way Turing was treated'. Queen Elizabeth II granted a pardon in 2013. The term 'Alan Turing law' is used informally to refer to a 2017 law in the UK that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.",
  "Turing left an extensive legacy in mathematics and computing which today is recognised more widely, with statues and many things named after him, including an annual award for computing innovation. His portrait appears on the Bank of England 50 note, first released on 23 June 2021 to coincide with his birthday. The audience vote in a 2019 BBC series named Turing the greatest person of the 20th century."
]

matrix, positions = incidence_matrix(texts)
print(search("turing AND computer", matrix, positions))
[1, 0, 1, 0, 0]

-------------------------

from nltk.corpus import stopwords
from collections import defaultdict

def stopword_removal(text):
  stopwords_set = set(stopwords.words('english'))
  cleaned = ''.join(char for char in text if char.isalnum() or char.isspace())
  tokens = cleaned.lower().split()
  filtered_tokens = [token for token in tokens if token not in stopwords_set]
  return ' '.join(filtered_tokens)

def inverted_index(texts):
  index = defaultdict(list)
  for i in range(len(texts)):
    texts[i] = set(stopword_removal(texts[i]).split()) # set for removing duplicate entries in postings
    for token in texts[i]: index[token].append(i+1)
  index = dict(sorted(index.items()))
  return index

texts = [
  "Alan Mathison Turing (23 June 1912 to 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science.",
  "Born in London, Turing was raised in southern England. He graduated from King's College, Cambridge, and in 1938, earned a doctorate degree from Princeton University. During World War II, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. He led Hut 8, the section responsible for German naval cryptanalysis. Turing devised techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. He played a crucial role in cracking intercepted messages that enabled the Allies to defeat the Axis powers in many engagements, including the Battle of the Atlantic.",
  "After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. Turing wrote on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, he was never fully recognised during his lifetime because much of his work was covered by the Official Secrets Act.",
  "In 1952, Turing was prosecuted for homosexual acts. He accepted hormone treatment, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, aged 41, from cyanide poisoning. An inquest determined his death as suicide, but the evidence is also consistent with accidental poisoning. Following a campaign in 2009, British prime minister Gordon Brown made an official public apology for 'the appalling way Turing was treated'. Queen Elizabeth II granted a pardon in 2013. The term 'Alan Turing law' is used informally to refer to a 2017 law in the UK that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.",
  "Turing left an extensive legacy in mathematics and computing which today is recognised more widely, with statues and many things named after him, including an annual award for computing innovation. His portrait appears on the Bank of England 50 note, first released on 23 June 2021 to coincide with his birthday. The audience vote in a 2019 BBC series named Turing the greatest person of the 20th century."
]

index = inverted_index(texts)
for term in index:
  print(f'{term}: {index[term]}')

-------------------------

def levenshtein_distance(s1, s2):
  rows, cols = len(s2) + 1, len(s1) + 1
  grid = [[0 for _ in range(cols)] for _ in range(rows)]

  for i in range(1, rows): grid[i][0] = i
  for j in range(1, cols): grid[0][j] = j

  for i in range(1, rows):
    for j in range(1, cols):
      deletion = grid[i-1][j] + 1
      insertion = grid[i][j-1] + 1
      substitution = grid[i-1][j-1] + (0 if s1[j-1] == s2[i-1] else 1)
      grid[i][j] = min(deletion, min(insertion, substitution))
  return grid[-1][-1]
    
s1 = "execution"
s2 = "intention"
distance = levenshtein_distance(s1, s2)
print(f"Levenshtein Distance: {distance}") # 5

-------------------------

def ngrams(sentence, n):
  result = []
  words = [f"${word}$" for word in sentence.split()]
  for word in words:
    for i in range(len(word) - n + 1):
      result.append(word[i:i+n])
  return result

def nwords(sentence, n):
  result = []
  words = sentence.split()
  for i in range(len(words) - n + 1):
    result.append(' '.join(words[i:i+n]))
  return result

n = 4
sentence = "war turing worked national physical laboratory designed automatic computing engine one first designs"
grams = ngrams(sentence, n)
words = nwords(sentence, n)
print(grams)

-------------------------

import numpy as np

def pagerank(graph, beta):
  nodes = list(graph.keys())
  n = len(nodes)
  positions = { node: index for index, node in enumerate(nodes) }

  adjacency = np.zeros((n, n))
  chances = np.ones((n, n)) / n

  for source, destinations in graph.items():
    for destination in destinations:
      adjacency[positions[destination], positions[source]] = 1

  column_sums = adjacency.sum(axis=0) # column
  for col in range(n):
    if column_sums[col] == 0: # no outgoing link
      adjacency[:, col] = 1 / n
    else:
      adjacency[:, col] /= column_sums[col]

  matrix = (beta * adjacency) + ((1 - beta) * chances)
    
  ranks = np.ones(n) / n
  while True:
    new_ranks = matrix.dot(ranks)
    if np.linalg.norm(new_ranks - ranks, 1) < 0.000001: break
    ranks = new_ranks
  
  for i, node in enumerate(nodes): print(f"{node}: {ranks[i]:.2f}")

graph = {
  'y': ['y', 'a'],
  'a': ['y', 'm'],
  'm': ['a'],
}
beta = 0.85
pagerank(graph, beta)

y: 0.38
a: 0.40
m: 0.22

-------------------------

from nltk.stem import PorterStemmer

def stem(sentence):
  words = sentence.split()
  stemmer = PorterStemmer()
  result = [stemmer.stem(word) for word in words]
  return ' '.join(result)

sentence = "Asseses the case with careful integration and combination"
print(f"Sentence: {sentence}")
print(f"Stemmed: {stem(sentence)}")

-------------------------

def soundex(s):
  s = s.upper()
  result = [s[0]] # keep first letter as it is
  mapping = {
    ('A', 'E', 'I', 'O', 'U', 'H', 'W', 'Y'): '0',
    ('B', 'F', 'P', 'V'): '1',
    ('C', 'G', 'J', 'K', 'Q', 'S', 'X', 'Z'): '2',
    ('D', 'T'): '3',
    ('L',): '4',
    ('M', 'N'): '5',
    ('R',): '6',
  }

  encoded = []
  for char in s[1:]: # skip first letter
    for key in mapping:
      if char in key:
        encoded.append(mapping[key])
        break

  filtered = []
  for i in range(len(encoded)):
    if i == 0 or encoded[i] != encoded[i - 1]: # skip duplicates
      filtered.append(encoded[i])

  result += [char for char in filtered if char != '0'] # remove zeroes
  while len(result) < 4: result.append('0') # pad to 4
  return ''.join(result[:4]) # return first 4
    
words = ["HERMAN", "HERMANN", "ASHCROFT", "ASHCRAFT", "PFISTER", "UNDERSTANDING"]
for s in words:
  print(f"{s}: {soundex(s)}")

-------------------------

from nltk.corpus import stopwords

# run once to download all stopwords 
# nltk.download('stopwords')

def stopword_removal(text):
  stopwords_set = set(stopwords.words('english'))
  cleaned = ''.join(char for char in text if char.isalnum() or char.isspace())
  tokens = cleaned.lower().split()
  filtered_tokens = [token for token in tokens if token not in stopwords_set]
  return ' '.join(filtered_tokens)

original = 'Alan Mathison Turing (23 June 1912 to 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science.'
# can also use file handling to read and write
print(stopword_removal(original))

-------------------------

import math
import numpy as np
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity

def get_tokens(tfs):
  tokens = set()
  for tf in tfs: tokens.update(tf.keys())
  return sorted(tokens)

def get_df(tfs, tokens):
  df = defaultdict(int)
  for token in tokens:
    for tf in tfs:
      if tf[token] > 0: df[token] += 1
  return df

def get_idf(dfs, n):
  idf = {}
  for token, df in dfs.items():
    idf[token] = round(math.log10(n / df) if df > 0 else 0.0, 2)
  return idf

def get_tfidf(tf, idf):
  tfidf = {}
  for token, freq in tf.items():
    tfidf[token] = round(math.log10(freq + 1) * idf.get(token, 0.0), 2)
  return tfidf

tfs = [
  { 'blue': 1, 'bright': 0, 'sky': 1, 'sun': 0 },
  { 'blue': 0, 'bright': 1, 'sky': 0, 'sun': 1 },
  { 'blue': 0, 'bright': 1, 'sky': 1, 'sun': 1 },
  { 'blue': 0, 'bright': 1, 'sky': 0, 'sun': 2 },
]
n = len(tfs)

tokens = get_tokens(tfs)
dfs = get_df(tfs, tokens)
idf = get_idf(dfs, n)
tfidf = [get_tfidf(tf, idf) for tf in tfs]

for i, weight in enumerate(tfidf):
  score = sum(weight.values())
  print(f"Doc {i+1} TF-IDF: {weight}")
  print(f"Doc {i+1} Score: {score}\n") # 0.27, 0.08, 0.17, 0,1

vectors = []
for doc in tfidf:
  vector = [doc.get(token, 0.0) for token in tokens]
  vectors.append(vector)

vectors = np.array(vectors)
cos_sim_matrix = cosine_similarity(vectors)
print("Cosine Similarity Matrix:")
print(cos_sim_matrix)